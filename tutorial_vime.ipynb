{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIME Tutorial\n",
    "\n",
    "### VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain\n",
    "\n",
    "- Paper: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar, \n",
    "  \"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,\" \n",
    "  Neural Information Processing Systems (NeurIPS), 2020.\n",
    "\n",
    "- Paper link: TBD\n",
    "\n",
    "- Last updated Date: October 11th 2020\n",
    "\n",
    "- Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "\n",
    "This notebook describes the user-guide of self- and semi-supervised learning for tabular domain using MNIST database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "Clone https://github.com/jsyoon0823/VIME.git to the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary packages and functions call\n",
    "\n",
    "- data_loader: MNIST dataset loading and preprocessing\n",
    "- supervised_models: supervised learning models (Logistic regression, XGBoost, and Multi-layer Perceptron)\n",
    "\n",
    "- vime_self: Self-supervised learning part of VIME framework\n",
    "- vime_semi: Semi-supervised learning part of VIME framework\n",
    "- vime_utils: Some utility functions for VIME framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "from data_loader import load_mnist_data\n",
    "from supervised_models import logit, xgb_model, mlp\n",
    "\n",
    "from vime_utils import perf_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters and define output\n",
    "\n",
    "-   label_no: Number of labeled data to be used\n",
    "-   model_sets: supervised model set (mlp, logit, or xgboost)\n",
    "-   p_m: corruption probability for self-supervised learning\n",
    "-   alpha: hyper-parameter to control the weights of feature and mask losses\n",
    "-   K: number of augmented samples\n",
    "-   beta: hyperparameter to control supervised and unsupervised loss\n",
    "-   label_data_rate: ratio of labeled data\n",
    "-   metric: prediction performance metric (either acc or auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental parameters\n",
    "label_no = 1000  \n",
    "model_sets = ['logit','xgboost','mlp']\n",
    "  \n",
    "# Hyper-parameters\n",
    "p_m = 0.3\n",
    "alpha = 2.0\n",
    "K = 3\n",
    "beta = 1.0\n",
    "label_data_rate = 0.1\n",
    "\n",
    "# Metric\n",
    "metric = 'acc'\n",
    "  \n",
    "# Define output\n",
    "results = np.zeros([len(model_sets)+2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Load original MNIST dataset and preprocess the loaded data.\n",
    "- Only select the subset of data as the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train, y_train, x_unlab, x_test, y_test = load_mnist_data(label_data_rate)\n",
    "    \n",
    "# Use subset of labeled data\n",
    "x_train = x_train[:label_no, :]\n",
    "y_train = y_train[:label_no, :]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train supervised models\n",
    "\n",
    "- Train 3 supervised learning models (Logistic regression, XGBoost, MLP)\n",
    "- Save the performances of each supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "y_test_hat = logit(x_train, y_train, x_test)\n",
    "results[0] = perf_metric(metric, y_test, y_test_hat) \n",
    "\n",
    "# XGBoost\n",
    "y_test_hat = xgb_model(x_train, y_train, x_test)    \n",
    "results[1] = perf_metric(metric, y_test, y_test_hat)   \n",
    "\n",
    "# MLP\n",
    "mlp_parameters = dict()\n",
    "mlp_parameters['hidden_dim'] = 100\n",
    "mlp_parameters['epochs'] = 100\n",
    "mlp_parameters['activation'] = 'relu'\n",
    "mlp_parameters['batch_size'] = 100\n",
    "      \n",
    "y_test_hat = mlp(x_train, y_train, x_test, mlp_parameters)\n",
    "results[2] = perf_metric(metric, y_test, y_test_hat)\n",
    "\n",
    "# Report performance\n",
    "for m_it in range(len(model_sets)):  \n",
    "    \n",
    "  model_name = model_sets[m_it]  \n",
    "    \n",
    "  print('Supervised Performance, Model Name: ' + model_name + \n",
    "        ', Performance: ' + str(results[m_it]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test VIME-Self\n",
    "Train self-supervised part of VIME framework only\n",
    "- Check the performance of self-supervised part of VIME framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain (VIME) Codebase.\n",
    "\n",
    "Reference: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar, \n",
    "\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2020.\n",
    "Paper link: TBD\n",
    "Last updated Date: October 11th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "\n",
    "vime_self.py\n",
    "- Self-supervised learning parts of the VIME framework\n",
    "- Using unlabeled data to train the encoder\n",
    "\"\"\"\n",
    "\n",
    "# Necessary packages\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "\n",
    "from vime_utils import mask_generator, pretext_generator\n",
    "\n",
    "\n",
    "def vime_self (x_unlab, p_m, alpha, parameters):\n",
    "  \"\"\"Self-supervised learning part in VIME.\n",
    "  \n",
    "  Args:\n",
    "    x_unlab: unlabeled feature\n",
    "    p_m: corruption probability\n",
    "    alpha: hyper-parameter to control the weights of feature and mask losses\n",
    "    parameters: epochs, batch_size\n",
    "    \n",
    "  Returns:\n",
    "    encoder: Representation learning block\n",
    "  \"\"\"\n",
    "    \n",
    "  # Parameters\n",
    "  _, dim = x_unlab.shape\n",
    "  epochs = parameters['epochs']\n",
    "  batch_size = parameters['batch_size']\n",
    "  \n",
    "  # Build model  \n",
    "  inputs = Input(shape=(dim,))\n",
    "  # Encoder  \n",
    "  h = Dense(int(dim), activation='relu')(inputs)  \n",
    "  # Mask estimator\n",
    "  output_1 = Dense(dim, activation='sigmoid', name = 'mask')(h)  \n",
    "  # Feature estimator\n",
    "  output_2 = Dense(dim, activation='sigmoid', name = 'feature')(h)\n",
    "  \n",
    "  model = Model(inputs = inputs, outputs = [output_1, output_2])\n",
    "  \n",
    "  model.compile(optimizer='rmsprop',\n",
    "                loss={'mask': 'binary_crossentropy', \n",
    "                      'feature': 'mean_squared_error'},\n",
    "                loss_weights={'mask':1.0, 'feature':alpha})\n",
    "  \n",
    "  # Generate corrupted samples\n",
    "  m_unlab = mask_generator(p_m, x_unlab)\n",
    "  m_label, x_tilde = pretext_generator(m_unlab, x_unlab)\n",
    "  \n",
    "  # Fit model on unlabeled data\n",
    "  model.fit(x_tilde, {'mask': m_label, 'feature': x_unlab}, \n",
    "            epochs = epochs, batch_size= batch_size)\n",
    "      \n",
    "  # Extract encoder part\n",
    "  layer_name = model.layers[1].name\n",
    "  layer_output = model.get_layer(layer_name).output\n",
    "  encoder = models.Model(inputs=model.input, outputs=layer_output)\n",
    "  \n",
    "  return encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VIME-Self\n",
    "vime_self_parameters = dict()\n",
    "vime_self_parameters['batch_size'] = 128\n",
    "vime_self_parameters['epochs'] = 10\n",
    "vime_self_encoder = vime_self(x_unlab, p_m, alpha, vime_self_parameters)\n",
    "  \n",
    "# Save encoder\n",
    "if not os.path.exists('save_model'):\n",
    "  os.makedirs('save_model')\n",
    "\n",
    "file_name = './save_model/encoder_model.h5'\n",
    "  \n",
    "vime_self_encoder.save(file_name)  \n",
    "        \n",
    "# Test VIME-Self\n",
    "x_train_hat = vime_self_encoder.predict(x_train)\n",
    "x_test_hat = vime_self_encoder.predict(x_test)\n",
    "      \n",
    "y_test_hat = mlp(x_train_hat, y_train, x_test_hat, mlp_parameters)\n",
    "results[3] = perf_metric(metric, y_test, y_test_hat)\n",
    "    \n",
    "print('VIME-Self Performance: ' + str(results[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test VIME\n",
    "\n",
    "Train semi-supervised part of VIME framework on top of trained self-supervised encoder\n",
    "- Check the performance of entire part of VIME framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain (VIME) Codebase.\n",
    "\n",
    "Reference: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar, \n",
    "\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2020.\n",
    "Paper link: TBD\n",
    "Last updated Date: October 11th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "\n",
    "vime_semi.py\n",
    "- Semi-supervised learning parts of the VIME framework\n",
    "- Using both labeled and unlabeled data to train the predictor with the help of trained encoder\n",
    "\"\"\"\n",
    "\n",
    "# Necessary packages\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.contrib import layers as contrib_layers\n",
    "# if using tensorflow 2.0, use the following\n",
    "from tensorflow.keras import layers as contrib_layers\n",
    "\n",
    "from vime_utils import mask_generator, pretext_generator\n",
    "\n",
    "\n",
    "def vime_semi(x_train, y_train, x_unlab, x_test, parameters, \n",
    "              p_m, K, beta, file_name):\n",
    "  \"\"\"Semi-supervied learning part in VIME.\n",
    "  \n",
    "  Args:\n",
    "    - x_train, y_train: training dataset\n",
    "    - x_unlab: unlabeled dataset\n",
    "    - x_test: testing features\n",
    "    - parameters: network parameters (hidden_dim, batch_size, iterations)\n",
    "    - p_m: corruption probability\n",
    "    - K: number of augmented samples\n",
    "    - beta: hyperparameter to control supervised and unsupervised loss\n",
    "    - file_name: saved filed name for the encoder function\n",
    "    \n",
    "  Returns:\n",
    "    - y_test_hat: prediction on x_test\n",
    "  \"\"\"\n",
    "      \n",
    "  # Network parameters\n",
    "  hidden_dim = parameters['hidden_dim']\n",
    "  act_fn = tf.nn.relu\n",
    "  batch_size = parameters['batch_size']\n",
    "  iterations = parameters['iterations']\n",
    "\n",
    "  # Basic parameters\n",
    "  data_dim = len(x_train[0, :])\n",
    "  label_dim = len(y_train[0, :])\n",
    "  \n",
    "  # Divide training and validation sets (9:1)\n",
    "  idx = np.random.permutation(len(x_train[:, 0]))\n",
    "  train_idx = idx[:int(len(idx)*0.9)]\n",
    "  valid_idx = idx[int(len(idx)*0.9):]\n",
    "  \n",
    "  x_valid = x_train[valid_idx, :]\n",
    "  y_valid = y_train[valid_idx, :]\n",
    "  \n",
    "  x_train = x_train[train_idx, :]\n",
    "  y_train = y_train[train_idx, :]  \n",
    "\n",
    "  # Input placeholder\n",
    "  # Labeled data\n",
    "  #x_input = tf.placeholder(tf.float32, [None, data_dim])\n",
    "  #y_input = tf.placeholder(tf.float32, [None, label_dim])\n",
    "  \n",
    "  # Augmented unlabeled data\n",
    "  #xu_input = tf.placeholder(tf.float32, [None, None, data_dim])\n",
    "\n",
    "  # in tensorflow 2.0, use the following\n",
    "  x_input = tf.keras.Input(shape=(data_dim,))\n",
    "  y_input = tf.keras.Input(shape=(label_dim,))\n",
    "  xu_input = tf.keras.Input(shape=(None, data_dim))\n",
    "  \n",
    "  ## Predictor\n",
    "  def predictor(x_input):\n",
    "    \"\"\"Returns prediction.\n",
    "    \n",
    "    Args: \n",
    "      - x_input: input feature\n",
    "      \n",
    "    Returns:\n",
    "      - y_hat_logit: logit prediction\n",
    "      - y_hat: prediction\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.name_scope('predictor'):     \n",
    "      # Stacks multi-layered perceptron\n",
    "      #inter_layer = contrib_layers.fully_connected(x_input, \n",
    "      #                                             hidden_dim, \n",
    "       #                                            activation_fn=act_fn)\n",
    "      #inter_layer = contrib_layers.fully_connected(inter_layer, \n",
    "        #                                           hidden_dim, \n",
    "         #                                          activation_fn=act_fn)\n",
    "\n",
    "      #y_hat_logit = contrib_layers.fully_connected(inter_layer, \n",
    "          #                                         label_dim, \n",
    "           #                                        activation_fn=None)\n",
    "      #y_hat = tf.nn.softmax(y_hat_logit)\n",
    "\n",
    "      # if using tensorflow 2.0, use the following\n",
    "      inter_layer = contrib_layers.Dense(hidden_dim, activation=act_fn)(x_input)\n",
    "      inter_layer = contrib_layers.Dense(hidden_dim, activation=act_fn)(inter_layer)\n",
    "      y_hat_logit = contrib_layers.Dense(label_dim, activation=None)(inter_layer)\n",
    "      #y_hat = tf.nn.softmax(y_hat_logit)\n",
    "      y_hat = tf.keras.activations.softmax(y_hat_logit)\n",
    "\n",
    "    return y_hat_logit, y_hat\n",
    "\n",
    "  # Build model\n",
    "  y_hat_logit, y_hat = predictor(x_input)    \n",
    "  yv_hat_logit, yv_hat = predictor(xu_input)\n",
    "  \n",
    "  # Defin losses\n",
    "  # Supervised loss\n",
    "  #y_loss = tf.losses.softmax_cross_entropy(y_input, y_hat_logit)\n",
    "  # if using tensorflow 2.0, use the following\n",
    "  y_loss = tf.keras.losses.CategoricalCrossentropy(y_input, y_hat_logit)  \n",
    "  # Unsupervised loss\n",
    "  # Assuming yv_hat_logit is a KerasTensor\n",
    "  yv_hat_logit_tensor = tf.convert_to_tensor(yv_hat_logit)\n",
    "  yu_loss = tf.math.reduce_mean(tf.nn.moments(yv_hat_logit_tensor, axes=0)[1])\n",
    "  #yu_loss = tf.math.reduce_mean(tf.nn.moments(yv_hat_logit, axes = 0)[1])\n",
    "  # if using tensorflow 2.0, use the following\n",
    "\n",
    "\n",
    "  # Define variables\n",
    "  p_vars = [v for v in tf.trainable_variables() \\\n",
    "            if v.name.startswith('predictor')]    \n",
    "  # Define solver\n",
    "  solver = tf.train.AdamOptimizer().minimize(y_loss + \\\n",
    "                                 beta * yu_loss, var_list=p_vars)\n",
    "\n",
    "  # Load encoder from self-supervised model\n",
    "  encoder = keras.models.load_model(file_name)\n",
    "  \n",
    "  # Encode validation and testing features\n",
    "  x_valid = encoder.predict(x_valid)  \n",
    "  x_test = encoder.predict(x_test)\n",
    "\n",
    "  # Start session\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "  # Setup early stopping procedure\n",
    "  class_file_name = './save_model/class_model.ckpt'\n",
    "  saver = tf.train.Saver(p_vars)\n",
    "    \n",
    "  yv_loss_min = 1e10\n",
    "  yv_loss_min_idx = -1\n",
    "  \n",
    "  # Training iteration loop\n",
    "  for it in range(iterations):\n",
    "\n",
    "    # Select a batch of labeled data\n",
    "    batch_idx = np.random.permutation(len(x_train[:, 0]))[:batch_size]\n",
    "    x_batch = x_train[batch_idx, :]\n",
    "    y_batch = y_train[batch_idx, :]    \n",
    "    \n",
    "    # Encode labeled data\n",
    "    x_batch = encoder.predict(x_batch)  \n",
    "    \n",
    "    # Select a batch of unlabeled data\n",
    "    batch_u_idx = np.random.permutation(len(x_unlab[:, 0]))[:batch_size]\n",
    "    xu_batch_ori = x_unlab[batch_u_idx, :]\n",
    "    \n",
    "    # Augment unlabeled data\n",
    "    xu_batch = list()\n",
    "    \n",
    "    for rep in range(K):      \n",
    "      # Mask vector generation\n",
    "      m_batch = mask_generator(p_m, xu_batch_ori)\n",
    "      # Pretext generator\n",
    "      _, xu_batch_temp = pretext_generator(m_batch, xu_batch_ori)\n",
    "      \n",
    "      # Encode corrupted samples\n",
    "      xu_batch_temp = encoder.predict(xu_batch_temp)\n",
    "      xu_batch = xu_batch + [xu_batch_temp]\n",
    "    # Convert list to matrix\n",
    "    xu_batch = np.asarray(xu_batch)\n",
    "\n",
    "    # Train the model\n",
    "    _, y_loss_curr = sess.run([solver, y_loss], \n",
    "                              feed_dict={x_input: x_batch, y_input: y_batch, \n",
    "                                         xu_input: xu_batch})  \n",
    "    # Current validation loss\n",
    "    yv_loss_curr = sess.run(y_loss, feed_dict={x_input: x_valid, \n",
    "                                               y_input: y_valid})\n",
    "  \n",
    "    if it % 100 == 0:\n",
    "      print('Iteration: ' + str(it) + '/' + str(iterations) + \n",
    "            ', Current loss: ' + str(np.round(yv_loss_curr, 4)))      \n",
    "      \n",
    "    # Early stopping & Best model save\n",
    "    if yv_loss_min > yv_loss_curr:\n",
    "      yv_loss_min = yv_loss_curr\n",
    "      yv_loss_min_idx = it\n",
    "\n",
    "      # Saves trained model\n",
    "      saver.save(sess, class_file_name)\n",
    "      \n",
    "    if yv_loss_min_idx + 100 < it:\n",
    "      break\n",
    "\n",
    "  #%% Restores the saved model\n",
    "  imported_graph = tf.train.import_meta_graph(class_file_name + '.meta')\n",
    "  \n",
    "  sess = tf.Session()\n",
    "  imported_graph.restore(sess, class_file_name)\n",
    "    \n",
    "  # Predict on x_test\n",
    "  y_test_hat = sess.run(y_hat, feed_dict={x_input: x_test})\n",
    "  \n",
    "  return y_test_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain (VIME) Codebase.\n",
    "\n",
    "Reference: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar, \n",
    "\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2020.\n",
    "Paper link: TBD\n",
    "Last updated Date: October 11th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "\n",
    "vime_semi.py\n",
    "- Semi-supervised learning parts of the VIME framework\n",
    "- Using both labeled and unlabeled data to train the predictor with the help of trained encoder\n",
    "\"\"\"\n",
    "\n",
    "# Necessary packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from vime_utils import mask_generator, pretext_generator\n",
    "\n",
    "def vime_semi(x_train, y_train, x_unlab, x_test, parameters, \n",
    "              p_m, K, beta, file_name):\n",
    "  \"\"\"Semi-supervied learning part in VIME.\n",
    "  \n",
    "  Args:\n",
    "    - x_train, y_train: training dataset\n",
    "    - x_unlab: unlabeled dataset\n",
    "    - x_test: testing features\n",
    "    - parameters: network parameters (hidden_dim, batch_size, iterations)\n",
    "    - p_m: corruption probability\n",
    "    - K: number of augmented samples\n",
    "    - beta: hyperparameter to control supervised and unsupervised loss\n",
    "    - file_name: saved filed name for the encoder function\n",
    "    \n",
    "  Returns:\n",
    "    - y_test_hat: prediction on x_test\n",
    "  \"\"\"\n",
    "      \n",
    "  # Network parameters\n",
    "  hidden_dim = parameters['hidden_dim']\n",
    "  act_fn = tf.nn.relu\n",
    "  batch_size = parameters['batch_size']\n",
    "  iterations = parameters['iterations']\n",
    "\n",
    "  # Basic parameters\n",
    "  data_dim = x_train.shape[1]\n",
    "  label_dim = y_train.shape[1]\n",
    "  \n",
    "  # Divide training and validation sets (9:1)\n",
    "  idx = np.random.permutation(len(x_train))\n",
    "  train_idx = idx[:int(len(idx)*0.9)]\n",
    "  valid_idx = idx[int(len(idx)*0.9):]\n",
    "  \n",
    "  x_valid = x_train[valid_idx]\n",
    "  y_valid = y_train[valid_idx]\n",
    "  \n",
    "  x_train = x_train[train_idx]\n",
    "  y_train = y_train[train_idx]\n",
    "\n",
    "  # Define predictor model\n",
    "  def build_predictor():\n",
    "    model = models.Sequential([\n",
    "      layers.Input(shape=(data_dim,)),\n",
    "      layers.Dense(hidden_dim, activation=act_fn),\n",
    "      layers.Dense(hidden_dim, activation=act_fn),\n",
    "      layers.Dense(label_dim, activation=None)\n",
    "    ])\n",
    "    return model\n",
    "  \n",
    "  predictor = build_predictor()\n",
    "\n",
    "  # Compile the predictor model\n",
    "  optimizer = optimizers.Adam()\n",
    "  predictor.compile(optimizer=optimizer, \n",
    "                    loss=losses.CategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "  # Load encoder from self-supervised model\n",
    "  encoder = models.load_model(file_name)\n",
    "  \n",
    "  # Encode validation and testing features\n",
    "  x_valid_encoded = encoder.predict(x_valid)\n",
    "  x_test_encoded = encoder.predict(x_test)\n",
    "  \n",
    "  best_loss = float('inf')\n",
    "  early_stop_counter = 0\n",
    "  \n",
    "  # Training iteration loop\n",
    "  for it in range(iterations):\n",
    "\n",
    "    # Select a batch of labeled data\n",
    "    batch_idx = np.random.permutation(len(x_train))[:batch_size]\n",
    "    x_batch = x_train[batch_idx]\n",
    "    y_batch = y_train[batch_idx]    \n",
    "    \n",
    "    # Encode labeled data\n",
    "    x_batch_encoded = encoder.predict(x_batch)  \n",
    "    \n",
    "    # Select a batch of unlabeled data\n",
    "    batch_u_idx = np.random.permutation(len(x_unlab))[:batch_size]\n",
    "    xu_batch_ori = x_unlab[batch_u_idx]\n",
    "    \n",
    "    # Augment unlabeled data\n",
    "    xu_batch = []\n",
    "    \n",
    "    for _ in range(K):\n",
    "      # Mask vector generation\n",
    "      m_batch = mask_generator(p_m, xu_batch_ori)\n",
    "      # Pretext generator\n",
    "      _, xu_batch_temp = pretext_generator(m_batch, xu_batch_ori)\n",
    "      \n",
    "      # Encode corrupted samples\n",
    "      xu_batch_temp_encoded = encoder.predict(xu_batch_temp)\n",
    "      xu_batch.append(xu_batch_temp_encoded)\n",
    "    \n",
    "    # Convert list to numpy array\n",
    "    xu_batch = np.array(xu_batch)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_hat_logit = predictor(x_batch_encoded, training=True)\n",
    "      y_loss = tf.reduce_mean(losses.categorical_crossentropy(y_batch, y_hat_logit, from_logits=True))\n",
    "      \n",
    "      yv_hat_logit = predictor(xu_batch, training=True)\n",
    "      yu_loss = tf.reduce_mean(tf.nn.moments(yv_hat_logit, axes=0)[1])\n",
    "      \n",
    "      loss = y_loss + beta * yu_loss\n",
    "\n",
    "    grads = tape.gradient(loss, predictor.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, predictor.trainable_variables))\n",
    "\n",
    "    # Current validation loss\n",
    "    yv_hat_logit_valid = predictor(x_valid_encoded, training=False)\n",
    "    yv_loss = tf.reduce_mean(losses.categorical_crossentropy(y_valid, yv_hat_logit_valid, from_logits=True))\n",
    "\n",
    "    if it % 100 == 0:\n",
    "      print(f'Iteration: {it}/{iterations}, Current loss: {yv_loss.numpy()}')\n",
    "      \n",
    "    # Early stopping & Best model save\n",
    "    if yv_loss < best_loss:\n",
    "      best_loss = yv_loss\n",
    "      early_stop_counter = 0\n",
    "      predictor.save_weights('best_predictor.h5')\n",
    "    else:\n",
    "      early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter > 100:\n",
    "      break\n",
    "\n",
    "  # Load the best model\n",
    "  predictor.load_weights('best_predictor.h5')\n",
    "  \n",
    "  # Predict on x_test\n",
    "  y_test_hat_logit = predictor(x_test_encoded, training=False)\n",
    "  y_test_hat = tf.nn.softmax(y_test_hat_logit).numpy()\n",
    "  \n",
    "  return y_test_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain (VIME) Codebase.\n",
    "\n",
    "Reference: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar, \n",
    "\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2020.\n",
    "Paper link: TBD\n",
    "Last updated Date: October 11th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "\n",
    "vime_semi.py\n",
    "- Semi-supervised learning parts of the VIME framework\n",
    "- Using both labeled and unlabeled data to train the predictor with the help of trained encoder\n",
    "\"\"\"\n",
    "\n",
    "# Necessary packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from vime_utils import mask_generator, pretext_generator\n",
    "\n",
    "def vime_semi(x_train, y_train, x_unlab, x_test, parameters, \n",
    "              p_m, K, beta, file_name):\n",
    "  \"\"\"Semi-supervied learning part in VIME.\n",
    "  \n",
    "  Args:\n",
    "    - x_train, y_train: training dataset\n",
    "    - x_unlab: unlabeled dataset\n",
    "    - x_test: testing features\n",
    "    - parameters: network parameters (hidden_dim, batch_size, iterations)\n",
    "    - p_m: corruption probability\n",
    "    - K: number of augmented samples\n",
    "    - beta: hyperparameter to control supervised and unsupervised loss\n",
    "    - file_name: saved filed name for the encoder function\n",
    "    \n",
    "  Returns:\n",
    "    - y_test_hat: prediction on x_test\n",
    "  \"\"\"\n",
    "      \n",
    "  # Network parameters\n",
    "  hidden_dim = parameters['hidden_dim']\n",
    "  act_fn = tf.nn.relu\n",
    "  batch_size = parameters['batch_size']\n",
    "  iterations = parameters['iterations']\n",
    "\n",
    "  # Basic parameters\n",
    "  data_dim = x_train.shape[1]\n",
    "  label_dim = y_train.shape[1]\n",
    "  \n",
    "  # Divide training and validation sets (9:1)\n",
    "  idx = np.random.permutation(len(x_train))\n",
    "  train_idx = idx[:int(len(idx)*0.9)]\n",
    "  valid_idx = idx[int(len(idx)*0.9):]\n",
    "  \n",
    "  x_valid = x_train[valid_idx]\n",
    "  y_valid = y_train[valid_idx]\n",
    "  \n",
    "  x_train = x_train[train_idx]\n",
    "  y_train = y_train[train_idx]\n",
    "\n",
    "  # Define predictor model\n",
    "  def build_predictor():\n",
    "    model = models.Sequential([\n",
    "      layers.Input(shape=(data_dim,)),\n",
    "      layers.Dense(hidden_dim, activation=act_fn),\n",
    "      layers.Dense(hidden_dim, activation=act_fn),\n",
    "      layers.Dense(label_dim, activation=None)\n",
    "    ])\n",
    "    return model\n",
    "  \n",
    "  predictor = build_predictor()\n",
    "\n",
    "  # Compile the predictor model\n",
    "  optimizer = optimizers.Adam()\n",
    "  predictor.compile(optimizer=optimizer, \n",
    "                    loss=losses.CategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "  # Load encoder from self-supervised model\n",
    "  encoder = models.load_model(file_name)\n",
    "  \n",
    "  # Encode validation and testing features\n",
    "  x_valid_encoded = encoder.predict(x_valid)\n",
    "  x_test_encoded = encoder.predict(x_test)\n",
    "  \n",
    "  best_loss = float('inf')\n",
    "  early_stop_counter = 0\n",
    "  \n",
    "  # Training iteration loop\n",
    "  for it in range(iterations):\n",
    "\n",
    "    # Select a batch of labeled data\n",
    "    batch_idx = np.random.permutation(len(x_train))[:batch_size]\n",
    "    x_batch = x_train[batch_idx]\n",
    "    y_batch = y_train[batch_idx]    \n",
    "    \n",
    "    # Encode labeled data\n",
    "    x_batch_encoded = encoder.predict(x_batch)  \n",
    "    \n",
    "    # Select a batch of unlabeled data\n",
    "    batch_u_idx = np.random.permutation(len(x_unlab))[:batch_size]\n",
    "    xu_batch_ori = x_unlab[batch_u_idx]\n",
    "    \n",
    "    # Augment unlabeled data\n",
    "    xu_batch = []\n",
    "    \n",
    "    for _ in range(K):\n",
    "      # Mask vector generation\n",
    "      m_batch = mask_generator(p_m, xu_batch_ori)\n",
    "      # Pretext generator\n",
    "      _, xu_batch_temp = pretext_generator(m_batch, xu_batch_ori)\n",
    "      \n",
    "      # Encode corrupted samples\n",
    "      xu_batch_temp_encoded = encoder.predict(xu_batch_temp)\n",
    "      xu_batch.append(xu_batch_temp_encoded)\n",
    "    \n",
    "    # Convert list to numpy array and reshape\n",
    "    xu_batch = np.concatenate(xu_batch, axis=0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_hat_logit = predictor(x_batch_encoded, training=True)\n",
    "      y_loss = tf.reduce_mean(losses.categorical_crossentropy(y_batch, y_hat_logit, from_logits=True))\n",
    "      \n",
    "      yv_hat_logit = predictor(xu_batch, training=True)\n",
    "      yu_loss = tf.reduce_mean(tf.nn.moments(yv_hat_logit, axes=0)[1])\n",
    "      \n",
    "      loss = y_loss + beta * yu_loss\n",
    "\n",
    "    grads = tape.gradient(loss, predictor.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, predictor.trainable_variables))\n",
    "\n",
    "    # Current validation loss\n",
    "    yv_hat_logit_valid = predictor(x_valid_encoded, training=False)\n",
    "    yv_loss = tf.reduce_mean(losses.categorical_crossentropy(y_valid, yv_hat_logit_valid, from_logits=True))\n",
    "\n",
    "    if it % 100 == 0:\n",
    "      print(f'Iteration: {it}/{iterations}, Current loss: {yv_loss.numpy()}')\n",
    "      \n",
    "    # Early stopping & Best model save\n",
    "    if yv_loss < best_loss:\n",
    "      best_loss = yv_loss\n",
    "      early_stop_counter = 0\n",
    "      predictor.save_weights('best_predictor.h5')\n",
    "    else:\n",
    "      early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter > 100:\n",
    "      break\n",
    "\n",
    "  # Load the best model\n",
    "  predictor.load_weights('best_predictor.h5')\n",
    "  \n",
    "  # Predict on x_test\n",
    "  y_test_hat_logit = predictor(x_test_encoded, training=False)\n",
    "  y_test_hat = tf.nn.softmax(y_test_hat_logit).numpy()\n",
    "  \n",
    "  return y_test_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain (VIME) Codebase.\n",
    "\n",
    "Reference: Jinsung Yoon, Yao Zhang, James Jordon, Mihaela van der Schaar, \n",
    "\"VIME: Extending the Success of Self- and Semi-supervised Learning to Tabular Domain,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2020.\n",
    "Paper link: TBD\n",
    "Last updated Date: October 11th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "\n",
    "vime_semi.py\n",
    "- Semi-supervised learning parts of the VIME framework\n",
    "- Using both labeled and unlabeled data to train the predictor with the help of trained encoder\n",
    "\"\"\"\n",
    "\n",
    "# Necessary packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "from vime_utils import mask_generator, pretext_generator\n",
    "\n",
    "def vime_semi(x_train, y_train, x_unlab, x_test, parameters, \n",
    "              p_m, K, beta, file_name):\n",
    "  \"\"\"Semi-supervied learning part in VIME.\n",
    "  \n",
    "  Args:\n",
    "    - x_train, y_train: training dataset\n",
    "    - x_unlab: unlabeled dataset\n",
    "    - x_test: testing features\n",
    "    - parameters: network parameters (hidden_dim, batch_size, iterations)\n",
    "    - p_m: corruption probability\n",
    "    - K: number of augmented samples\n",
    "    - beta: hyperparameter to control supervised and unsupervised loss\n",
    "    - file_name: saved filed name for the encoder function\n",
    "    \n",
    "  Returns:\n",
    "    - y_test_hat: prediction on x_test\n",
    "  \"\"\"\n",
    "      \n",
    "  # Network parameters\n",
    "  hidden_dim = parameters['hidden_dim']\n",
    "  act_fn = tf.nn.relu\n",
    "  batch_size = parameters['batch_size']\n",
    "  iterations = parameters['iterations']\n",
    "\n",
    "  # Basic parameters\n",
    "  data_dim = x_train.shape[1]\n",
    "  label_dim = y_train.shape[1]\n",
    "  \n",
    "  # Divide training and validation sets (9:1)\n",
    "  idx = np.random.permutation(len(x_train))\n",
    "  train_idx = idx[:int(len(idx)*0.9)]\n",
    "  valid_idx = idx[int(len(idx)*0.9):]\n",
    "  \n",
    "  x_valid = x_train[valid_idx]\n",
    "  y_valid = y_train[valid_idx]\n",
    "  \n",
    "  x_train = x_train[train_idx]\n",
    "  y_train = y_train[train_idx]\n",
    "\n",
    "  # Define predictor model\n",
    "  def build_predictor():\n",
    "    model = models.Sequential([\n",
    "      layers.Input(shape=(data_dim,)),\n",
    "      layers.Dense(hidden_dim, activation=act_fn),\n",
    "      layers.Dense(hidden_dim, activation=act_fn),\n",
    "      layers.Dense(label_dim, activation=None)\n",
    "    ])\n",
    "    return model\n",
    "  \n",
    "  predictor = build_predictor()\n",
    "\n",
    "  # Compile the predictor model\n",
    "  optimizer = optimizers.Adam()\n",
    "  predictor.compile(optimizer=optimizer, \n",
    "                    loss=losses.CategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "  # Load encoder from self-supervised model\n",
    "  encoder = models.load_model(file_name)\n",
    "  \n",
    "  # Encode validation and testing features\n",
    "  x_valid_encoded = encoder.predict(x_valid)\n",
    "  x_test_encoded = encoder.predict(x_test)\n",
    "  \n",
    "  best_loss = float('inf')\n",
    "  early_stop_counter = 0\n",
    "  \n",
    "  # Training iteration loop\n",
    "  for it in range(iterations):\n",
    "\n",
    "    # Select a batch of labeled data\n",
    "    batch_idx = np.random.permutation(len(x_train))[:batch_size]\n",
    "    x_batch = x_train[batch_idx]\n",
    "    y_batch = y_train[batch_idx]    \n",
    "    \n",
    "    # Encode labeled data\n",
    "    x_batch_encoded = encoder.predict(x_batch)  \n",
    "    \n",
    "    # Select a batch of unlabeled data\n",
    "    batch_u_idx = np.random.permutation(len(x_unlab))[:batch_size]\n",
    "    xu_batch_ori = x_unlab[batch_u_idx]\n",
    "    \n",
    "    # Augment unlabeled data\n",
    "    xu_batch = []\n",
    "    \n",
    "    for _ in range(K):\n",
    "      # Mask vector generation\n",
    "      m_batch = mask_generator(p_m, xu_batch_ori)\n",
    "      # Pretext generator\n",
    "      _, xu_batch_temp = pretext_generator(m_batch, xu_batch_ori)\n",
    "      \n",
    "      # Encode corrupted samples\n",
    "      xu_batch_temp_encoded = encoder.predict(xu_batch_temp)\n",
    "      xu_batch.append(xu_batch_temp_encoded)\n",
    "    \n",
    "    # Convert list to numpy array and reshape\n",
    "    xu_batch = np.concatenate(xu_batch, axis=0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_hat_logit = predictor(x_batch_encoded, training=True)\n",
    "      y_loss = tf.reduce_mean(losses.categorical_crossentropy(y_batch, y_hat_logit, from_logits=True))\n",
    "      \n",
    "      yv_hat_logit = predictor(xu_batch, training=True)\n",
    "      yu_loss = tf.reduce_mean(tf.nn.moments(yv_hat_logit, axes=0)[1])\n",
    "      \n",
    "      loss = y_loss + beta * yu_loss\n",
    "\n",
    "    grads = tape.gradient(loss, predictor.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, predictor.trainable_variables))\n",
    "\n",
    "    # Current validation loss\n",
    "    yv_hat_logit_valid = predictor(x_valid_encoded, training=False)\n",
    "    yv_loss = tf.reduce_mean(losses.categorical_crossentropy(y_valid, yv_hat_logit_valid, from_logits=True))\n",
    "\n",
    "    if it % 100 == 0:\n",
    "      print(f'Iteration: {it}/{iterations}, Current loss: {yv_loss.numpy()}')\n",
    "      \n",
    "    # Early stopping & Best model save\n",
    "    if yv_loss < best_loss:\n",
    "      best_loss = yv_loss\n",
    "      early_stop_counter = 0\n",
    "      predictor.save_weights('best_predictor.weights.h5')\n",
    "    else:\n",
    "      early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter > 100:\n",
    "      break\n",
    "\n",
    "  # Load the best model\n",
    "  predictor.load_weights('best_predictor.weights.h5')\n",
    "  \n",
    "  # Predict on x_test\n",
    "  y_test_hat_logit = predictor(x_test_encoded, training=False)\n",
    "  y_test_hat = tf.nn.softmax(y_test_hat_logit).numpy()\n",
    "  \n",
    "  return y_test_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VIME-Semi\n",
    "vime_semi_parameters = dict()\n",
    "vime_semi_parameters['hidden_dim'] = 100\n",
    "vime_semi_parameters['batch_size'] = 128\n",
    "vime_semi_parameters['iterations'] = 1000\n",
    "y_test_hat = vime_semi(x_train, y_train, x_unlab, x_test, \n",
    "                       vime_semi_parameters, p_m, K, beta, file_name)\n",
    "\n",
    "# Test VIME\n",
    "results[4] = perf_metric(metric, y_test, y_test_hat)\n",
    "  \n",
    "print('VIME Performance: '+ str(results[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Prediction Performances\n",
    "\n",
    "- 3 Supervised learning models\n",
    "- VIME with self-supervised part only\n",
    "- Entire VIME framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m_it in range(len(model_sets)):  \n",
    "    \n",
    "  model_name = model_sets[m_it]  \n",
    "    \n",
    "  print('Supervised Performance, Model Name: ' + model_name + \n",
    "        ', Performance: ' + str(results[m_it]))\n",
    "    \n",
    "print('VIME-Self Performance: ' + str(results[m_it+1]))\n",
    "  \n",
    "print('VIME Performance: '+ str(results[m_it+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
